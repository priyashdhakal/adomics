{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Gene1     Gene2     Gene3     Gene4\n",
      "Gene1  1.000000 -1.000000 -0.188982  0.400000\n",
      "Gene2 -1.000000  1.000000  0.188982 -0.400000\n",
      "Gene3 -0.188982  0.188982  1.000000 -0.755929\n",
      "Gene4  0.400000 -0.400000 -0.755929  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have gene expression data in a DataFrame where rows are genes and columns are samples\n",
    "# Replace this with your actual gene expression data\n",
    "gene_expression_data = pd.DataFrame({\n",
    "    'Gene1': [1, 2, 3, 4, 5],\n",
    "    'Gene2': [5, 4, 3, 2, 1],\n",
    "    'Gene3': [3, 2, 1, 3, 2],\n",
    "    'Gene4': [2, 3, 4, 1, 5]\n",
    "})\n",
    "\n",
    "def co_expression_similarity(data):\n",
    "    \"\"\"\n",
    "    Calculate co-expression similarity of genes using Pearson correlation coefficient\n",
    "    :param data: DataFrame where rows are genes and columns are samples\n",
    "    :return: DataFrame containing pairwise co-expression similarity between genes\n",
    "    \"\"\"\n",
    "    return data.corr(method='pearson')\n",
    "\n",
    "# Calculate co-expression similarity\n",
    "co_expression_similarity_matrix = co_expression_similarity(gene_expression_data)\n",
    "print(co_expression_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted Adjacency Matrix:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "Weighted Adjacency Matrix:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def threshold_adjacency(similarity_matrix, threshold):\n",
    "    \"\"\"\n",
    "    Calculate unweighted and weighted adjacency matrices from a similarity matrix using a threshold\n",
    "    :param similarity_matrix: DataFrame containing pairwise similarities between genes\n",
    "    :param threshold: Threshold value to determine which similarities should be considered as edges\n",
    "    :return: Tuple containing unweighted and weighted adjacency matrices\n",
    "    \"\"\"\n",
    "    # Initialize matrices\n",
    "    num_genes = similarity_matrix.shape[0]\n",
    "    unweighted_adjacency = np.zeros((num_genes, num_genes))\n",
    "    weighted_adjacency = np.zeros((num_genes, num_genes))\n",
    "\n",
    "    # Calculate unweighted and weighted adjacency matrices\n",
    "    for i in range(num_genes):\n",
    "        for j in range(num_genes):\n",
    "            similarity = similarity_matrix.iloc[i, j]\n",
    "            if similarity >= threshold:\n",
    "                unweighted_adjacency[i, j] = 1\n",
    "                weighted_adjacency[i, j] = similarity\n",
    "\n",
    "    return unweighted_adjacency, weighted_adjacency\n",
    "\n",
    "# Example usage\n",
    "threshold_value = 0.5  # Set your threshold value here\n",
    "unweighted_adjacency_matrix, weighted_adjacency_matrix = threshold_adjacency(co_expression_similarity_matrix, threshold_value)\n",
    "\n",
    "print(\"Unweighted Adjacency Matrix:\")\n",
    "print(unweighted_adjacency_matrix)\n",
    "print(\"\\nWeighted Adjacency Matrix:\")\n",
    "print(weighted_adjacency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[100, 100], edge_index=[2, 1000], y=[100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15393/2524008286.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  edge_index = torch.tensor(adjacency_matrix.nonzero(), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# Example gene expression data (replace this with your actual omics data)\n",
    "omics_data = torch.randn(100, 500)  # Assuming 100 samples and 500 features (genes)\n",
    "\n",
    "# Define a placeholder for the target variable (if applicable)\n",
    "target_variable = torch.randint(0, 2, (100,))  # Assuming binary classification with 100 samples\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "omics_data_normalized = scaler.fit_transform(omics_data.numpy())\n",
    "\n",
    "# Feature selection (optional)\n",
    "selector = SelectKBest(score_func=mutual_info_regression, k=100)\n",
    "omics_data_selected = selector.fit_transform(omics_data_normalized, target_variable)\n",
    "\n",
    "\n",
    "# Construct a K-nearest neighbor graph\n",
    "adjacency_matrix = kneighbors_graph(omics_data_selected, n_neighbors=10, mode='connectivity').toarray()\n",
    "\n",
    "# Convert adjacency matrix to edge index format\n",
    "edge_index = torch.tensor(adjacency_matrix.nonzero(), dtype=torch.long)\n",
    "\n",
    "# Create a PyG Data object\n",
    "graph_data = Data(x=torch.tensor(omics_data_selected, dtype=torch.float32),\n",
    "                  edge_index=edge_index,\n",
    "                  y=target_variable)\n",
    "\n",
    "print(graph_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhakal/anaconda3/envs/GOA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2708, 1433]), torch.Size([2, 10556]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape, data.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(data.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 7])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(data.x, data.edge_index)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9463, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9463\n",
      "Epoch: 002, Loss: 1.9409\n",
      "Epoch: 003, Loss: 1.9343\n",
      "Epoch: 004, Loss: 1.9275\n",
      "Epoch: 005, Loss: 1.9181\n",
      "Epoch: 006, Loss: 1.9086\n",
      "Epoch: 007, Loss: 1.9015\n",
      "Epoch: 008, Loss: 1.8933\n",
      "Epoch: 009, Loss: 1.8808\n",
      "Epoch: 010, Loss: 1.8685\n",
      "Epoch: 011, Loss: 1.8598\n",
      "Epoch: 012, Loss: 1.8482\n",
      "Epoch: 013, Loss: 1.8290\n",
      "Epoch: 014, Loss: 1.8233\n",
      "Epoch: 015, Loss: 1.8057\n",
      "Epoch: 016, Loss: 1.7966\n",
      "Epoch: 017, Loss: 1.7825\n",
      "Epoch: 018, Loss: 1.7617\n",
      "Epoch: 019, Loss: 1.7491\n",
      "Epoch: 020, Loss: 1.7310\n",
      "Epoch: 021, Loss: 1.7147\n",
      "Epoch: 022, Loss: 1.7056\n",
      "Epoch: 023, Loss: 1.6954\n",
      "Epoch: 024, Loss: 1.6697\n",
      "Epoch: 025, Loss: 1.6538\n",
      "Epoch: 026, Loss: 1.6312\n",
      "Epoch: 027, Loss: 1.6161\n",
      "Epoch: 028, Loss: 1.5899\n",
      "Epoch: 029, Loss: 1.5711\n",
      "Epoch: 030, Loss: 1.5576\n",
      "Epoch: 031, Loss: 1.5393\n",
      "Epoch: 032, Loss: 1.5137\n",
      "Epoch: 033, Loss: 1.4948\n",
      "Epoch: 034, Loss: 1.4913\n",
      "Epoch: 035, Loss: 1.4698\n",
      "Epoch: 036, Loss: 1.3998\n",
      "Epoch: 037, Loss: 1.4041\n",
      "Epoch: 038, Loss: 1.3761\n",
      "Epoch: 039, Loss: 1.3631\n",
      "Epoch: 040, Loss: 1.3258\n",
      "Epoch: 041, Loss: 1.3030\n",
      "Epoch: 042, Loss: 1.3119\n",
      "Epoch: 043, Loss: 1.2519\n",
      "Epoch: 044, Loss: 1.2530\n",
      "Epoch: 045, Loss: 1.2492\n",
      "Epoch: 046, Loss: 1.2205\n",
      "Epoch: 047, Loss: 1.2037\n",
      "Epoch: 048, Loss: 1.1571\n",
      "Epoch: 049, Loss: 1.1700\n",
      "Epoch: 050, Loss: 1.1296\n",
      "Epoch: 051, Loss: 1.0860\n",
      "Epoch: 052, Loss: 1.1080\n",
      "Epoch: 053, Loss: 1.0564\n",
      "Epoch: 054, Loss: 1.0157\n",
      "Epoch: 055, Loss: 1.0362\n",
      "Epoch: 056, Loss: 1.0328\n",
      "Epoch: 057, Loss: 1.0058\n",
      "Epoch: 058, Loss: 0.9865\n",
      "Epoch: 059, Loss: 0.9667\n",
      "Epoch: 060, Loss: 0.9741\n",
      "Epoch: 061, Loss: 0.9769\n",
      "Epoch: 062, Loss: 0.9122\n",
      "Epoch: 063, Loss: 0.8993\n",
      "Epoch: 064, Loss: 0.8769\n",
      "Epoch: 065, Loss: 0.8575\n",
      "Epoch: 066, Loss: 0.8897\n",
      "Epoch: 067, Loss: 0.8312\n",
      "Epoch: 068, Loss: 0.8262\n",
      "Epoch: 069, Loss: 0.8511\n",
      "Epoch: 070, Loss: 0.7711\n",
      "Epoch: 071, Loss: 0.8012\n",
      "Epoch: 072, Loss: 0.7529\n",
      "Epoch: 073, Loss: 0.7525\n",
      "Epoch: 074, Loss: 0.7689\n",
      "Epoch: 075, Loss: 0.7553\n",
      "Epoch: 076, Loss: 0.7032\n",
      "Epoch: 077, Loss: 0.7326\n",
      "Epoch: 078, Loss: 0.7122\n",
      "Epoch: 079, Loss: 0.7090\n",
      "Epoch: 080, Loss: 0.6755\n",
      "Epoch: 081, Loss: 0.6666\n",
      "Epoch: 082, Loss: 0.6679\n",
      "Epoch: 083, Loss: 0.7037\n",
      "Epoch: 084, Loss: 0.6752\n",
      "Epoch: 085, Loss: 0.6266\n",
      "Epoch: 086, Loss: 0.6564\n",
      "Epoch: 087, Loss: 0.6266\n",
      "Epoch: 088, Loss: 0.6411\n",
      "Epoch: 089, Loss: 0.6226\n",
      "Epoch: 090, Loss: 0.6535\n",
      "Epoch: 091, Loss: 0.6317\n",
      "Epoch: 092, Loss: 0.5741\n",
      "Epoch: 093, Loss: 0.5572\n",
      "Epoch: 094, Loss: 0.5710\n",
      "Epoch: 095, Loss: 0.5816\n",
      "Epoch: 096, Loss: 0.5745\n",
      "Epoch: 097, Loss: 0.5547\n",
      "Epoch: 098, Loss: 0.5989\n",
      "Epoch: 099, Loss: 0.6021\n",
      "Epoch: 100, Loss: 0.5799\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GOA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
